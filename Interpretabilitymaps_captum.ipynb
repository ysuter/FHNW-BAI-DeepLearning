{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysuter/FHNW-BAI-DeepLearning/blob/main/Interpretabilitymaps_captum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLlYoTKDIp4U"
      },
      "source": [
        "# üîç Saliency Maps & Interpretierbarkeit mit Captum\n",
        "\n",
        "**Lernziele:**\n",
        "- Verstehen, was Saliency Maps zeigen und was nicht\n",
        "- Verschiedene Attributionsmethoden kennenlernen (Saliency, Integrated Gradients, GradCAM)\n",
        "- Kritisch reflektieren: Sind diese Erkl√§rungen \"echt\"?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzgIWDPPIp4V"
      },
      "source": [
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUve7xHeIp4W"
      },
      "outputs": [],
      "source": [
        "# Captum installieren (Facebooks Interpretability Library f√ºr PyTorch)\n",
        "!pip install captum -q\n",
        "!pip install numpy==2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBdDdKEKIp4X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Captum Imports\n",
        "from captum.attr import Saliency, IntegratedGradients, GuidedGradCam, LayerGradCam\n",
        "from captum.attr import visualization as viz\n",
        "\n",
        "# F√ºr reproduzierbare Ergebnisse\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Verwendetes Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQITqsNSIp4X"
      },
      "source": [
        "## 2. Modell laden\n",
        "\n",
        "Wir verwenden **ResNet-18** ‚Äì ein relativ kleines CNN mit 18 Schichten. Es wurde auf ImageNet (1000 Klassen, 1.2 Mio. Bilder) trainiert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oL6av7XIp4X"
      },
      "outputs": [],
      "source": [
        "# Vortrainiertes ResNet-18 laden\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model = model.to(device)\n",
        "model.eval()  # Wichtig: Evaluation-Modus f√ºr Inferenz\n",
        "\n",
        "print(f\"Modell geladen: {sum(p.numel() for p in model.parameters()):,} Parameter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Je-aSdmLIp4X"
      },
      "outputs": [],
      "source": [
        "# ImageNet Klassennamen laden\n",
        "LABELS_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "imagenet_classes = requests.get(LABELS_URL).text.strip().split(\"\\n\")\n",
        "print(f\"{len(imagenet_classes)} Klassen geladen. Beispiele: {imagenet_classes[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV2pHaD-Ip4Y"
      },
      "source": [
        "## 3. Hilfsfunktionen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk-lBitKIp4Y"
      },
      "outputs": [],
      "source": [
        "# Preprocessing f√ºr ImageNet-Modelle\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "def load_image(url_or_path):\n",
        "    \"\"\"L√§dt ein Bild von URL oder lokalem Pfad.\"\"\"\n",
        "    if url_or_path.startswith('http'):\n",
        "        response = requests.get(url_or_path)\n",
        "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    else:\n",
        "        img = Image.open(url_or_path).convert('RGB')\n",
        "    return img\n",
        "\n",
        "def predict(model, input_tensor):\n",
        "    \"\"\"Gibt Top-5 Vorhersagen zur√ºck.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        probs = F.softmax(output, dim=1)\n",
        "        top5_probs, top5_indices = probs.topk(5)\n",
        "\n",
        "    results = []\n",
        "    for prob, idx in zip(top5_probs[0], top5_indices[0]):\n",
        "        results.append((imagenet_classes[idx], prob.item()))\n",
        "    return results, top5_indices[0][0].item()\n",
        "\n",
        "def tensor_to_image(tensor):\n",
        "    \"\"\"Konvertiert normalisierten Tensor zur√ºck zu anzeigbarem Bild.\"\"\"\n",
        "    # Denormalisieren\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    img = tensor.cpu() * std + mean\n",
        "    img = img.squeeze().permute(1, 2, 0).detach().numpy()\n",
        "    return np.clip(img, 0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN11EnlWIp4Y"
      },
      "source": [
        "## 4. Testbild laden\n",
        "\n",
        "Wir starten mit einem klassischen Beispiel. Sp√§ter k√∂nnt ihr eigene Bilder testen!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21DsX2AsIp4Y"
      },
      "outputs": [],
      "source": [
        "# Beispielbilder zum Testen (einfach URL √§ndern oder eigenes Bild hochladen)\n",
        "EXAMPLE_IMAGES_old = {\n",
        "    \"hund\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/1200px-YellowLabradorLooking_new.jpg\",\n",
        "    \"katze\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\",\n",
        "    \"auto\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/2019_Toyota_Corolla_Icon_Tech_VVT-i_Hybrid_1.8.jpg/1200px-2019_Toyota_Corolla_Icon_Tech_VVT-i_Hybrid_1.8.jpg\",\n",
        "    \"vogel\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Eopsaltria_australis_-_Mogo_Campground.jpg/1200px-Eopsaltria_australis_-_Mogo_Campground.jpg\",\n",
        "    \"schmetterling\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Papilio_machaon_Mitterbach_01.jpg/1200px-Papilio_machaon_Mitterbach_01.jpg\"\n",
        "}\n",
        "\n",
        "EXAMPLE_IMAGES = {\n",
        "    \"hund\": \"https://images.unsplash.com/photo-1587300003388-59208cc962cb?w=800\",\n",
        "    \"katze\": \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=800\",\n",
        "    \"auto\": \"https://images.unsplash.com/photo-1494976388531-d1058494ceb8?w=800\",\n",
        "    \"vogel\": \"https://images.unsplash.com/photo-1444464666168-49d633b86797?w=800\",\n",
        "}\n",
        "\n",
        "# Bild ausw√§hlen\n",
        "selected_image = \"katze\"  # <-- HIER GE√ÑNDERT, um ein anderes Bild zu versuchen\n",
        "\n",
        "# Bild laden und vorbereiten\n",
        "original_image = load_image(EXAMPLE_IMAGES[selected_image])\n",
        "input_tensor = preprocess(original_image).unsqueeze(0).to(device)\n",
        "input_tensor.requires_grad = True  # Wichtig f√ºr Gradient-basierte Methoden!\n",
        "\n",
        "# Vorhersage\n",
        "predictions, top_class = predict(model, input_tensor)\n",
        "\n",
        "# Anzeigen\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(original_image)\n",
        "plt.title(f\"Top-Vorhersage: {predictions[0][0]} ({predictions[0][1]*100:.1f}%)\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop-5 Vorhersagen:\")\n",
        "for label, prob in predictions:\n",
        "    print(f\"  {label}: {prob*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE0FJJE5Ip4Y"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Attributionsmethoden\n",
        "\n",
        "Jetzt wird es spannend: Wir fragen das Modell \"Warum hast du so entschieden?\"\n",
        "\n",
        "### 5.1 Vanilla Saliency (Gradient)\n",
        "\n",
        "**Idee:** Berechne den Gradienten des Outputs bez√ºglich des Inputs. Pixel mit hohem Gradienten sind \"wichtig\" f√ºr die Entscheidung.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKVuIy6bIp4Z"
      },
      "outputs": [],
      "source": [
        "# Saliency berechnen\n",
        "saliency = Saliency(model)\n",
        "attribution_saliency = saliency.attribute(input_tensor, target=top_class)\n",
        "\n",
        "# Visualisierung\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Originalbild\n",
        "axes[0].imshow(tensor_to_image(input_tensor))\n",
        "axes[0].set_title(\"Original\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Saliency Map (Graustufen)\n",
        "saliency_np = attribution_saliency.squeeze().cpu().detach().numpy()\n",
        "saliency_gray = np.abs(saliency_np).max(axis=0)  # Max √ºber Farbkan√§le\n",
        "axes[1].imshow(saliency_gray, cmap='hot')\n",
        "axes[1].set_title(\"Saliency Map\")\n",
        "axes[1].axis('off')\n",
        "\n",
        "# Overlay\n",
        "axes[2].imshow(tensor_to_image(input_tensor))\n",
        "axes[2].imshow(saliency_gray, cmap='hot', alpha=0.5)\n",
        "axes[2].set_title(\"Overlay\")\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.suptitle(f\"Vanilla Saliency f√ºr Klasse: {imagenet_classes[top_class]}\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L1p-wu0Ip4Z"
      },
      "source": [
        "### üí¨ Diskussionsfrage\n",
        "\n",
        "> Schaut euch die Saliency Map an: Fokussiert das Modell auf die \"richtigen\" Stellen? Oder gibt es √ºberraschende Bereiche?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_YnRTk6Ip4Z"
      },
      "source": [
        "---\n",
        "\n",
        "### 5.2 Integrated Gradients\n",
        "\n",
        "**Problem mit Vanilla Saliency:** Gradienten k√∂nnen bei ges√§ttigten Funktionen verschwinden (Gradient Saturation).\n",
        "\n",
        "**L√∂sung:** Integriere Gradienten entlang eines Pfades von einer Baseline (z.B. schwarzes Bild) zum Input.\n",
        "\n",
        "**Formel:** $\\text{IG}(x) = (x - x') \\cdot \\int_0^1 \\frac{\\partial f(x' + \\alpha(x-x'))}{\\partial x} d\\alpha$\n",
        "\n",
        "Diese Methode erf√ºllt wichtige **Axiome**:\n",
        "- Sensitivity: Unterschiedliche Inputs ‚Üí unterschiedliche Attributionen\n",
        "- Implementation Invariance: Gleiche Funktion ‚Üí gleiche Attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KJeaV7WIp4Z"
      },
      "outputs": [],
      "source": [
        "# Integrated Gradients berechnen\n",
        "ig = IntegratedGradients(model)\n",
        "\n",
        "# Baseline: schwarzes Bild (Standard), alternativ: Rauschen, Blur, etc.\n",
        "baseline = torch.zeros_like(input_tensor).to(device)\n",
        "\n",
        "attribution_ig = ig.attribute(\n",
        "    input_tensor,\n",
        "    baselines=baseline,\n",
        "    target=top_class,\n",
        "    n_steps=50  # Anzahl der Integrationsschritte\n",
        ")\n",
        "\n",
        "# Visualisierung\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "axes[0].imshow(tensor_to_image(input_tensor))\n",
        "axes[0].set_title(\"Original\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "ig_np = attribution_ig.squeeze().cpu().detach().numpy()\n",
        "ig_gray = np.abs(ig_np).max(axis=0)\n",
        "axes[1].imshow(ig_gray, cmap='hot')\n",
        "axes[1].set_title(\"Integrated Gradients\")\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(tensor_to_image(input_tensor))\n",
        "axes[2].imshow(ig_gray, cmap='hot', alpha=0.5)\n",
        "axes[2].set_title(\"Overlay\")\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.suptitle(f\"Integrated Gradients f√ºr Klasse: {imagenet_classes[top_class]}\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYIh26V9Ip4Z"
      },
      "source": [
        "### üí° Experiment: Andere Baseline ausprobieren\n",
        "\n",
        "Die Wahl der Baseline beeinflusst die Attribution! Testet verschiedene Baselines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nobWkZ9sIp4Z"
      },
      "outputs": [],
      "source": [
        "# Verschiedene Baselines vergleichen\n",
        "baselines = {\n",
        "    \"Schwarz\": torch.zeros_like(input_tensor),\n",
        "    \"Weiss\": torch.ones_like(input_tensor),\n",
        "    \"Rauschen\": torch.randn_like(input_tensor) * 0.1,\n",
        "    \"Blur\": transforms.GaussianBlur(kernel_size=31, sigma=10)(input_tensor.cpu()).to(device)\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(1, len(baselines), figsize=(16, 4))\n",
        "\n",
        "for ax, (name, baseline) in zip(axes, baselines.items()):\n",
        "    attr = ig.attribute(input_tensor, baselines=baseline.to(device), target=top_class, n_steps=30)\n",
        "    attr_np = np.abs(attr.squeeze().cpu().detach().numpy()).max(axis=0)\n",
        "    ax.imshow(tensor_to_image(input_tensor))\n",
        "    ax.imshow(attr_np, cmap='hot', alpha=0.5)\n",
        "    ax.set_title(f\"Baseline: {name}\")\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle(\"Einfluss der Baseline auf Integrated Gradients\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2Em4JH_Ip4a"
      },
      "source": [
        "### üí¨ Diskussionsfrage\n",
        "\n",
        "> Welche Baseline ist die \"richtige\"? Gibt es √ºberhaupt eine richtige Antwort?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRIbFa4eIp4a"
      },
      "source": [
        "---\n",
        "\n",
        "### 5.3 GradCAM (Gradient-weighted Class Activation Mapping)\n",
        "\n",
        "**Idee:** Statt Pixel-level Attributionen schauen wir uns an, welche **Feature Maps** in einer bestimmten Schicht wichtig sind.\n",
        "\n",
        "**Vorteil:** Gr√∂bere, aber oft intuitivere Erkl√§rungen. Zeigt \"Regionen\" statt einzelner Pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bEIndVlIp4a"
      },
      "outputs": [],
      "source": [
        "# GradCAM auf der letzten Convolutional Layer\n",
        "gradcam = LayerGradCam(model, model.layer4[-1].conv2)  # Letzte Conv-Schicht von ResNet\n",
        "\n",
        "attribution_gradcam = gradcam.attribute(input_tensor, target=top_class)\n",
        "\n",
        "# GradCAM muss hochskaliert werden (ist nur 7x7 bei ResNet)\n",
        "gradcam_upsampled = F.interpolate(\n",
        "    attribution_gradcam,\n",
        "    size=(224, 224),\n",
        "    mode='bilinear',\n",
        "    align_corners=False\n",
        ")\n",
        "\n",
        "# Visualisierung\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "axes[0].imshow(tensor_to_image(input_tensor))\n",
        "axes[0].set_title(\"Original\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "gradcam_np = gradcam_upsampled.squeeze().cpu().detach().numpy()\n",
        "gradcam_np = np.maximum(gradcam_np, 0)  # ReLU\n",
        "axes[1].imshow(gradcam_np, cmap='jet')\n",
        "axes[1].set_title(\"GradCAM Heatmap\")\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(tensor_to_image(input_tensor))\n",
        "axes[2].imshow(gradcam_np, cmap='jet', alpha=0.5)\n",
        "axes[2].set_title(\"Overlay\")\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.suptitle(f\"GradCAM f√ºr Klasse: {imagenet_classes[top_class]}\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7UEp9dIIp4a"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Methodenvergleich\n",
        "\n",
        "Alle drei Methoden auf einen Blick:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zmITnu1Ip4a"
      },
      "outputs": [],
      "source": [
        "def compare_methods(input_tensor, target_class):\n",
        "    \"\"\"Vergleicht alle Attributionsmethoden nebeneinander.\"\"\"\n",
        "\n",
        "    # Berechnungen\n",
        "    saliency_attr = saliency.attribute(input_tensor, target=target_class)\n",
        "    ig_attr = ig.attribute(input_tensor, target=target_class, n_steps=50)\n",
        "    gradcam_attr = gradcam.attribute(input_tensor, target=target_class)\n",
        "    gradcam_up = F.interpolate(gradcam_attr, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "\n",
        "    # Normalisieren\n",
        "    sal_np = np.abs(saliency_attr.squeeze().cpu().detach().numpy()).max(axis=0)\n",
        "    ig_np = np.abs(ig_attr.squeeze().cpu().detach().numpy()).max(axis=0)\n",
        "    gc_np = np.maximum(gradcam_up.squeeze().cpu().detach().numpy(), 0)\n",
        "\n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
        "\n",
        "    axes[0].imshow(tensor_to_image(input_tensor))\n",
        "    axes[0].set_title(f\"Original\\n{imagenet_classes[target_class]}\")\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(tensor_to_image(input_tensor))\n",
        "    axes[1].imshow(sal_np, cmap='hot', alpha=0.6)\n",
        "    axes[1].set_title(\"Saliency\\n(Gradient)\")\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(tensor_to_image(input_tensor))\n",
        "    axes[2].imshow(ig_np, cmap='hot', alpha=0.6)\n",
        "    axes[2].set_title(\"Integrated Gradients\\n(Pfad-Integration)\")\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    axes[3].imshow(tensor_to_image(input_tensor))\n",
        "    axes[3].imshow(gc_np, cmap='jet', alpha=0.5)\n",
        "    axes[3].set_title(\"GradCAM\\n(Feature-Map-basiert)\")\n",
        "    axes[3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "compare_methods(input_tensor, top_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80PPlI8cIp4a"
      },
      "source": [
        "---\n",
        "\n",
        "## 7. üß™ Experimente zum Selbst-Ausprobieren\n",
        "\n",
        "### Experiment A: Andere Klasse als Ziel\n",
        "\n",
        "Was, wenn wir fragen: \"Wo im Bild sieht das Modell Hinweise auf eine ANDERE Klasse?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IKLR6vjIp4a"
      },
      "outputs": [],
      "source": [
        "# W√§hle eine andere Klasse als Ziel\n",
        "# Finde Index f√ºr eine bestimmte Klasse:\n",
        "def find_class(name):\n",
        "    matches = [(i, c) for i, c in enumerate(imagenet_classes) if name.lower() in c.lower()]\n",
        "    return matches[:10]\n",
        "\n",
        "# Suche nach Klassen\n",
        "print(\"Beispiel-Klassen mit 'cat':\")\n",
        "print(find_class(\"cat\"))\n",
        "\n",
        "print(\"\\nBeispiel-Klassen mit 'grass':\")\n",
        "print(find_class(\"grass\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YT7IzRrmIp4a"
      },
      "outputs": [],
      "source": [
        "# Vergleiche: Attribution f√ºr tats√§chliche Klasse vs. andere Klasse\n",
        "alternative_class = 281  # 281 = tabby cat (bei Hundebild interessant)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# GradCAM f√ºr Top-Prediction\n",
        "attr_top = gradcam.attribute(input_tensor, target=top_class)\n",
        "attr_top_up = F.interpolate(attr_top, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "axes[0].imshow(tensor_to_image(input_tensor))\n",
        "axes[0].imshow(np.maximum(attr_top_up.squeeze().cpu().detach().numpy(), 0), cmap='jet', alpha=0.5)\n",
        "axes[0].set_title(f\"GradCAM f√ºr: {imagenet_classes[top_class]}\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "# GradCAM f√ºr alternative Klasse\n",
        "attr_alt = gradcam.attribute(input_tensor, target=alternative_class)\n",
        "attr_alt_up = F.interpolate(attr_alt, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "axes[1].imshow(tensor_to_image(input_tensor))\n",
        "axes[1].imshow(np.maximum(attr_alt_up.squeeze().cpu().detach().numpy(), 0), cmap='jet', alpha=0.5)\n",
        "axes[1].set_title(f\"GradCAM f√ºr: {imagenet_classes[alternative_class]}\")\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljV2GqGcIp4b"
      },
      "source": [
        "### Experiment B: Eigenes Bild hochladen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KH19tITIp4b"
      },
      "outputs": [],
      "source": [
        "# In Google Colab: Bild hochladen\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Lade ein Bild hoch...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    custom_image = load_image(filename)\n",
        "    custom_tensor = preprocess(custom_image).unsqueeze(0).to(device)\n",
        "    custom_tensor.requires_grad = True\n",
        "\n",
        "    predictions, custom_top_class = predict(model, custom_tensor)\n",
        "    print(f\"\\nVorhersage: {predictions[0][0]} ({predictions[0][1]*100:.1f}%)\")\n",
        "\n",
        "    compare_methods(custom_tensor, custom_top_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPgsbCdGIp4b"
      },
      "source": [
        "### Experiment C: Andere Schicht f√ºr GradCAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMs1Bz1uIp4b"
      },
      "outputs": [],
      "source": [
        "# GradCAM auf verschiedenen Schichten von ResNet\n",
        "layers = {\n",
        "    \"Layer 1 (fr√ºh)\": model.layer1[-1].conv2,\n",
        "    \"Layer 2\": model.layer2[-1].conv2,\n",
        "    \"Layer 3\": model.layer3[-1].conv2,\n",
        "    \"Layer 4 (sp√§t)\": model.layer4[-1].conv2,\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(1, len(layers) + 1, figsize=(20, 4))\n",
        "\n",
        "axes[0].imshow(tensor_to_image(input_tensor))\n",
        "axes[0].set_title(\"Original\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "for ax, (name, layer) in zip(axes[1:], layers.items()):\n",
        "    gc = LayerGradCam(model, layer)\n",
        "    attr = gc.attribute(input_tensor, target=top_class)\n",
        "    attr_up = F.interpolate(attr, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "\n",
        "    ax.imshow(tensor_to_image(input_tensor))\n",
        "    ax.imshow(np.maximum(attr_up.squeeze().cpu().detach().numpy(), 0), cmap='jet', alpha=0.5)\n",
        "    ax.set_title(name)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle(\"GradCAM auf verschiedenen Netzwerk-Tiefen\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1jB9JyKIp4b"
      },
      "source": [
        "### üí¨ Diskussionsfrage\n",
        "\n",
        "> Fr√ºhe Schichten zeigen feinere Details, sp√§te Schichten gr√∂√üere Regionen. Welche Erkl√§rung ist \"besser\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6L6eHGuIp4b"
      },
      "source": [
        "---\n",
        "\n",
        "## 8. Kritische Reflexion\n",
        "\n",
        "### ‚ö†Ô∏è Grenzen von Saliency Maps\n",
        "\n",
        "1. **Keine Kausalit√§t:** Saliency Maps zeigen Korrelationen, nicht Ursachen\n",
        "2. **Instabilit√§t:** Kleine √Ñnderungen im Bild ‚Üí gro√üe √Ñnderungen in der Map\n",
        "3. **Faithfulness:** Erkl√§rt die Map wirklich das Modell oder nur unsere Erwartungen?\n",
        "4. **Baseline-Abh√§ngigkeit:** Bei IG beeinflusst die Baseline das Ergebnis massiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBkpKzVxIp4b"
      },
      "outputs": [],
      "source": [
        "# Demo: Instabilit√§t von Saliency Maps\n",
        "# Kleines Rauschen hinzuf√ºgen\n",
        "noise = torch.randn_like(input_tensor) * 0.01\n",
        "noisy_tensor = (input_tensor + noise).requires_grad_(True)\n",
        "\n",
        "# Saliency berechnen\n",
        "attr_original = saliency.attribute(input_tensor, target=top_class)\n",
        "attr_noisy = saliency.attribute(noisy_tensor, target=top_class)\n",
        "\n",
        "# Vergleich\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "sal_orig = np.abs(attr_original.squeeze().cpu().detach().numpy()).max(axis=0)\n",
        "sal_noisy = np.abs(attr_noisy.squeeze().cpu().detach().numpy()).max(axis=0)\n",
        "\n",
        "axes[0].imshow(sal_orig, cmap='hot')\n",
        "axes[0].set_title(\"Saliency: Original\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(sal_noisy, cmap='hot')\n",
        "axes[1].set_title(\"Saliency: +1% Rauschen\")\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(np.abs(sal_orig - sal_noisy), cmap='hot')\n",
        "axes[2].set_title(\"Differenz\")\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.suptitle(\"‚ö†Ô∏è Instabilit√§t: Minimales Rauschen ‚Üí Andere Erkl√§rung\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99gmjl7YIp4c"
      },
      "source": [
        "---\n",
        "\n",
        "## 9. Zusammenfassung\n",
        "\n",
        "| Methode | Vorteile | Nachteile |\n",
        "|---------|----------|------------|\n",
        "| **Saliency** | Schnell, einfach | \"Verrauscht\", instabil |\n",
        "| **Integrated Gradients** | Theoretisch fundiert, Axiome | Baseline-abh√§ngig, langsamer |\n",
        "| **GradCAM** | Intuitive Regionen, stabil | Grob, nur f√ºr CNNs |\n",
        "\n",
        "### üéØ Key Takeaways\n",
        "\n",
        "1. **Keine Methode ist perfekt** ‚Äì alle haben Trade-offs\n",
        "2. **Post-hoc ‚â† echtes Verstehen** ‚Äì wir approximieren nur\n",
        "3. **Mehrere Methoden kombinieren** f√ºr robustere Insights\n",
        "4. **Domain-Experten einbeziehen** ‚Äì nur sie k√∂nnen Plausibilit√§t bewerten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqUHgq-eIp4c"
      },
      "source": [
        "---\n",
        "\n",
        "## 10. Weiterf√ºhrende Ressourcen\n",
        "\n",
        "- [Captum Dokumentation](https://captum.ai/)\n",
        "- [Paper: \"Axiomatic Attribution for Deep Networks\"](https://arxiv.org/abs/1703.01365) (Integrated Gradients)\n",
        "- [Paper: \"Grad-CAM\"](https://arxiv.org/abs/1610.02391)\n",
        "- [Paper: \"Stop Explaining Black Box ML Models...\"](https://arxiv.org/abs/1811.10154) (Kritik an Post-hoc Erkl√§rungen)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}