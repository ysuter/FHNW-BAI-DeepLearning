{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysuter/FHNW-BAI-DeepLearning/blob/main/LLM_BusinessAI_Colab_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5833a152",
      "metadata": {
        "id": "5833a152"
      },
      "source": [
        "# Large Language Models\n",
        "\n",
        "You will:\n",
        "- Explore **tokenization** for language models\n",
        "- Generate text with a **small GPT-2 model**\n",
        "- Experiment with **temperature** and **top-k sampling**\n",
        "\n",
        "> üí° You can run this notebook on Google Colab. Just upload it and run the cells from top to bottom.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dd0eb2c",
      "metadata": {
        "id": "6dd0eb2c"
      },
      "source": [
        "## Learning goals\n",
        "\n",
        "By the end of this notebook, you should be able to:\n",
        "\n",
        "1. Explain how a **tokenizer** transforms text into model inputs.\n",
        "2. Run **inference** with a small, pretrained LLM (GPT-2).\n",
        "3. Understand how **temperature** and **top-k** affect generation.\n",
        "4. Critically reflect on when LLM outputs are **useful** vs. **unreliable** in business settings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8daa25c",
      "metadata": {
        "id": "d8daa25c"
      },
      "source": [
        "## 1Ô∏è‚É£ Setup\n",
        "\n",
        "Run the cell below to install and import the required libraries.\n",
        "\n",
        "If you're on Google Colab, this should work out of the box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3abbf8b1",
      "metadata": {
        "id": "3abbf8b1"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate sentencepiece torch\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbf43cf2",
      "metadata": {
        "id": "bbf43cf2"
      },
      "source": [
        "## 2Ô∏è‚É£ Tokenization: From text to tokens\n",
        "\n",
        "Language models don't see raw text. They see **tokens** ‚Äì integers that represent subwords, words, or sometimes bytes.\n",
        "\n",
        "In this section you will:\n",
        "\n",
        "- Inspect how the tokenizer splits a sentence\n",
        "- See the difference between **text**, **tokens**, and **token IDs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9037e670",
      "metadata": {
        "id": "9037e670"
      },
      "outputs": [],
      "source": [
        "# Load a tokenizer (GPT-2-style)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "text = \"Explain market segmentation in one sentence.\"\n",
        "print(\"Original text:\")\n",
        "print(text)\n",
        "print()\n",
        "\n",
        "# Tokenize\n",
        "encoded = tokenizer(text, return_tensors=\"pt\")\n",
        "input_ids = encoded[\"input_ids\"][0]\n",
        "\n",
        "print(\"Token IDs:\")\n",
        "print(input_ids.tolist())\n",
        "print()\n",
        "\n",
        "print(\"Tokens:\")\n",
        "print([tokenizer.decode([tid]) for tid in input_ids])\n",
        "print()\n",
        "\n",
        "print(f\"Number of tokens: {len(input_ids)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aba89adf",
      "metadata": {
        "id": "aba89adf"
      },
      "source": [
        "üëâ **Questions to discuss (or think about):**\n",
        "- Does the tokenizer split by **words**, **subwords**, or something else?\n",
        "- What happens if you change the input text slightly (e.g., add punctuation, numbers, emojis)?\n",
        "- Why might a business care about token length (hint: API costs, context window limits)?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3836fda",
      "metadata": {
        "id": "e3836fda"
      },
      "source": [
        "## 3Ô∏è‚É£ Generating text with a small LLM (GPT-2)\n",
        "\n",
        "Now we load a **pretrained GPT-2 model** and ask it to generate text.\n",
        "\n",
        "> ‚ö†Ô∏è GPT-2 is relatively small and **not instruction-tuned**, so its answers may be short, generic, or odd. That‚Äôs fine ‚Äì it‚Äôs perfect for learning the mechanics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e16310",
      "metadata": {
        "id": "a0e16310"
      },
      "outputs": [],
      "source": [
        "# Load a small GPT-2 model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "model.eval()\n",
        "\n",
        "prompt = \"Explain deep learning in simple terms:\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=40,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated text:\")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88a50e42",
      "metadata": {
        "id": "88a50e42"
      },
      "source": [
        "üëâ **Try this:**\n",
        "- Change the `prompt` to something else (e.g., *\"Describe the concept of churn in marketing analytics.\"*)\n",
        "- Increase `max_new_tokens` to 100. What happens?\n",
        "- Remove `do_sample=True` and sampling parameters. How does the output change?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cbf48e7",
      "metadata": {
        "id": "9cbf48e7"
      },
      "source": [
        "## 4Ô∏è‚É£ Experiment: Temperature and top-k sampling\n",
        "\n",
        "Two important knobs when generating text:\n",
        "\n",
        "- **Temperature**: controls how *random* the model is.\n",
        "  - Low temperature (e.g., 0.2) ‚Üí more **deterministic**, safer, but boring.\n",
        "  - High temperature (e.g., 1.2) ‚Üí more **creative**, but also more chaotic.\n",
        "- **Top-k**: the model only samples from the **k most likely** next tokens.\n",
        "  - Small k (e.g., 10) ‚Üí conservative.\n",
        "  - Large k (e.g., 100) ‚Üí more diverse.\n",
        "\n",
        "Let‚Äôs compare different settings side-by-side.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7658e18b",
      "metadata": {
        "id": "7658e18b"
      },
      "outputs": [],
      "source": [
        "def generate_with_settings(prompt, temperature=0.7, top_k=50, max_new_tokens=40):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "        )\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "prompt = \"Write a short product description for an AI-powered analytics dashboard for business managers.\"\n",
        "\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "for temp in [0.2, 0.7, 1.2]:\n",
        "    print(f\"--- Temperature = {temp} ---\")\n",
        "    text = generate_with_settings(prompt, temperature=temp, top_k=50)\n",
        "    print(text)\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f1a5501",
      "metadata": {
        "id": "3f1a5501"
      },
      "source": [
        "üëâ **Discussion:**\n",
        "- How do the outputs change as temperature increases?\n",
        "- Which output looks most useful for a **marketing website**?\n",
        "- Which output would you trust to send to a **client** without editing?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}