{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WfdeXpGT4tT"
      },
      "source": [
        "Variational Autoencoders (VAEs)\n",
        "======\n",
        "\n",
        "The VAE implemented here uses the setup found in most VAE papers: a multivariate Normal distribution for the conditional distribution of the latent vectors given and input image ($q_{\\phi}(z | x_i)$ in the slides) and a multivariate Bernoulli distribution for the conditional distribution of images given the latent vector ($p_{\\theta}(x | z)$ in the slides). Using a Bernoulli distribution, the reconstruction loss (negative log likelihood of a data point in the output distribution) reduces to the pixel-wise binary cross-entropy. See the [original VAE paper](https://arxiv.org/pdf/1312.6114.pdf), Appendix C.1 for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CzBcOq0QT4tW"
      },
      "outputs": [],
      "source": [
        "# install pytorch (http://pytorch.org/) if run from Google Colaboratory\n",
        "import sys\n",
        "%matplotlib inline\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR4ZdogET4tZ"
      },
      "source": [
        "Parameter Settings\n",
        "-------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oKPRaBvVT4tb"
      },
      "outputs": [],
      "source": [
        "# 2-d latent space, parameter count in same order of magnitude\n",
        "# as in the original VAE paper (VAE paper has about 3x as many)\n",
        "latent_dims = 2\n",
        "num_epochs = 100\n",
        "batch_size = 128\n",
        "capacity = 64\n",
        "learning_rate = 1e-3\n",
        "variational_beta = 1\n",
        "use_gpu = True\n",
        "\n",
        "# # 10-d latent space, for comparison with non-variational auto-encoder\n",
        "# latent_dims = 10\n",
        "# num_epochs = 100\n",
        "# batch_size = 128\n",
        "# capacity = 64\n",
        "# learning_rate = 1e-3\n",
        "# variational_beta = 1\n",
        "# use_gpu = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSdfW-tET4tc"
      },
      "source": [
        "MNIST Data Loading\n",
        "-------------------\n",
        "\n",
        "MNIST images show digits from 0-9 in 28x28 grayscale images. We do not center them at 0, because we will be using a binary cross-entropy loss that treats pixel values as probabilities in [0,1]. We create both a training set and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uRPM8moXT4td",
        "outputId": "c603b278-b390-42b1-c751-3ec2b5e84ad4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 13.4MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 341kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.17MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 14.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = MNIST(root='./data/MNIST', download=True, train=True, transform=img_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = MNIST(root='./data/MNIST', download=True, train=False, transform=img_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIa2vyQYT4te"
      },
      "source": [
        "VAE Definition\n",
        "-----------------------\n",
        "We use a convolutional encoder and decoder, which generally gives better performance than fully connected versions that have the same number of parameters.\n",
        "\n",
        "In convolution layers, we increase the channels as we approach the bottleneck, but note that the total number of features still decreases, since the channels increase by a factor of 2 in each convolution, but the spatial size decreases by a factor of 4.\n",
        "\n",
        "Kernel size 4 is used to avoid biasing problems described here: https://distill.pub/2016/deconv-checkerboard/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nygZ_DL-T4tg",
        "outputId": "17ebb5b7-0325-4250-f6f0-4ea17bc08b96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 308357\n"
          ]
        }
      ],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=c, kernel_size=4, stride=2, padding=1) # out: c x 14 x 14\n",
        "        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1) # out: c x 7 x 7\n",
        "        self.fc_mu = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)\n",
        "        self.fc_logvar = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n",
        "        x_mu = self.fc_mu(x)\n",
        "        x_logvar = self.fc_logvar(x)\n",
        "        return x_mu, x_logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*7*7)\n",
        "        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.size(0), capacity*2, 7, 7) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.sigmoid(self.conv1(x)) # last layer before output is sigmoid, since we are using BCE as reconstruction loss\n",
        "        return x\n",
        "\n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent_mu, latent_logvar = self.encoder(x)\n",
        "        latent = self.latent_sample(latent_mu, latent_logvar)\n",
        "        x_recon = self.decoder(latent)\n",
        "        return x_recon, latent_mu, latent_logvar\n",
        "\n",
        "    def latent_sample(self, mu, logvar):\n",
        "        if self.training:\n",
        "            # the reparameterization trick\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = torch.empty_like(std).normal_()\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    # recon_x is the probability of a multivariate Bernoulli distribution p.\n",
        "    # -log(p(x)) is then the pixel-wise binary cross-entropy.\n",
        "    # Averaging or not averaging the binary cross-entropy over all pixels here\n",
        "    # is a subtle detail with big effect on training, since it changes the weight\n",
        "    # we need to pick for the other loss term by several orders of magnitude.\n",
        "    # Not averaging is the direct implementation of the negative log likelihood,\n",
        "    # but averaging makes the weight of the other loss term independent of the image resolution.\n",
        "    recon_loss = F.binary_cross_entropy(recon_x.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
        "\n",
        "    # KL-divergence between the prior distribution over latent vectors\n",
        "    # (the one we are going to sample from when generating new images)\n",
        "    # and the distribution estimated by the generator for the given image.\n",
        "    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return recon_loss + variational_beta * kldivergence\n",
        "\n",
        "\n",
        "vae = VariationalAutoencoder()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "vae = vae.to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n",
        "print('Number of parameters: %d' % num_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KucwRNsT4ti"
      },
      "source": [
        "Train VAE\n",
        "--------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Enmw_VHjT4tk",
        "outputId": "f59ad62a-cb62-43f5-b7cd-fef8323a16d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ...\n",
            "Epoch [1 / 100] average reconstruction error: 23439.273063\n",
            "Epoch [2 / 100] average reconstruction error: 21150.559691\n",
            "Epoch [3 / 100] average reconstruction error: 20587.837312\n",
            "Epoch [4 / 100] average reconstruction error: 20293.069917\n",
            "Epoch [5 / 100] average reconstruction error: 20089.181441\n",
            "Epoch [6 / 100] average reconstruction error: 19936.185339\n",
            "Epoch [7 / 100] average reconstruction error: 19819.494084\n",
            "Epoch [8 / 100] average reconstruction error: 19725.346417\n",
            "Epoch [9 / 100] average reconstruction error: 19636.087932\n",
            "Epoch [10 / 100] average reconstruction error: 19563.152358\n",
            "Epoch [11 / 100] average reconstruction error: 19507.763235\n",
            "Epoch [12 / 100] average reconstruction error: 19456.627640\n",
            "Epoch [13 / 100] average reconstruction error: 19391.234958\n",
            "Epoch [14 / 100] average reconstruction error: 19344.710459\n",
            "Epoch [15 / 100] average reconstruction error: 19306.443099\n",
            "Epoch [16 / 100] average reconstruction error: 19262.791322\n",
            "Epoch [17 / 100] average reconstruction error: 19225.691475\n",
            "Epoch [18 / 100] average reconstruction error: 19193.172745\n",
            "Epoch [19 / 100] average reconstruction error: 19174.015338\n",
            "Epoch [20 / 100] average reconstruction error: 19139.195862\n",
            "Epoch [21 / 100] average reconstruction error: 19102.085119\n",
            "Epoch [22 / 100] average reconstruction error: 19075.559114\n",
            "Epoch [23 / 100] average reconstruction error: 19060.259341\n",
            "Epoch [24 / 100] average reconstruction error: 19034.638728\n",
            "Epoch [25 / 100] average reconstruction error: 19012.275830\n",
            "Epoch [26 / 100] average reconstruction error: 18979.491321\n",
            "Epoch [27 / 100] average reconstruction error: 18967.294303\n",
            "Epoch [28 / 100] average reconstruction error: 18944.461695\n",
            "Epoch [29 / 100] average reconstruction error: 18929.522147\n",
            "Epoch [30 / 100] average reconstruction error: 18915.972725\n",
            "Epoch [31 / 100] average reconstruction error: 18887.421921\n",
            "Epoch [32 / 100] average reconstruction error: 18861.516599\n",
            "Epoch [33 / 100] average reconstruction error: 18861.127975\n",
            "Epoch [34 / 100] average reconstruction error: 18842.066737\n",
            "Epoch [35 / 100] average reconstruction error: 18826.857443\n",
            "Epoch [36 / 100] average reconstruction error: 18805.534238\n",
            "Epoch [37 / 100] average reconstruction error: 18793.149789\n",
            "Epoch [38 / 100] average reconstruction error: 18779.138795\n",
            "Epoch [39 / 100] average reconstruction error: 18752.392176\n",
            "Epoch [40 / 100] average reconstruction error: 18757.302010\n",
            "Epoch [41 / 100] average reconstruction error: 18745.711743\n",
            "Epoch [42 / 100] average reconstruction error: 18722.252478\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# set to training mode\n",
        "vae.train()\n",
        "\n",
        "train_loss_avg = []\n",
        "\n",
        "print('Training ...')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "\n",
        "    for image_batch, _ in train_dataloader:\n",
        "\n",
        "        image_batch = image_batch.to(device)\n",
        "\n",
        "        # vae reconstruction\n",
        "        image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\n",
        "\n",
        "        # reconstruction error\n",
        "        loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)\n",
        "\n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # one step of the optmizer (using the gradients from backpropagation)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss_avg[-1] += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    train_loss_avg[-1] /= num_batches\n",
        "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqAjXKQMT4tl"
      },
      "source": [
        "Plot Training Curve\n",
        "--------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7p-GPXDT4tl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(train_loss_avg)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wvMXrJuT4tm"
      },
      "source": [
        "Alternatively: Load Pre-Trained VAE\n",
        "-----------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikDkPKe0T4tn"
      },
      "outputs": [],
      "source": [
        "filename = 'vae_2d.pth'\n",
        "# filename = 'vae_10d.pth'\n",
        "import urllib\n",
        "if not os.path.isdir('./pretrained'):\n",
        "    os.makedirs('./pretrained')\n",
        "print('downloading ...')\n",
        "urllib.request.urlretrieve (\"http://geometry.cs.ucl.ac.uk/creativeai/pretrained/\"+filename, \"./pretrained/\"+filename)\n",
        "vae.load_state_dict(torch.load('./pretrained/'+filename))\n",
        "print('done')\n",
        "\n",
        "# this is how the VAE parameters can be saved:\n",
        "# torch.save(vae.state_dict(), './pretrained/my_vae.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fETin61vT4tn"
      },
      "source": [
        "Evaluate on the Test Set\n",
        "-------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9plgdq6T4to"
      },
      "outputs": [],
      "source": [
        "# set to evaluation mode\n",
        "vae.eval()\n",
        "\n",
        "test_loss_avg, num_batches = 0, 0\n",
        "for image_batch, _ in test_dataloader:\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        image_batch = image_batch.to(device)\n",
        "\n",
        "        # vae reconstruction\n",
        "        image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\n",
        "\n",
        "        # reconstruction error\n",
        "        loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)\n",
        "\n",
        "        test_loss_avg += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "test_loss_avg /= num_batches\n",
        "print('average reconstruction error: %f' % (test_loss_avg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wjRNzpeT4tp"
      },
      "source": [
        "Visualize Reconstructions\n",
        "--------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqVaN18VT4tp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "import torchvision.utils\n",
        "\n",
        "vae.eval()\n",
        "\n",
        "# This function takes as an input the images to reconstruct\n",
        "# and the name of the model with which the reconstructions\n",
        "# are performed\n",
        "def to_img(x):\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "\n",
        "def show_image(img):\n",
        "    img = to_img(img)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def visualise_output(images, model):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        images = images.to(device)\n",
        "        images, _, _ = model(images)\n",
        "        images = images.cpu()\n",
        "        images = to_img(images)\n",
        "        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()\n",
        "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "images, labels = iter(test_dataloader).next()\n",
        "\n",
        "# First visualise the original images\n",
        "print('Original images')\n",
        "show_image(torchvision.utils.make_grid(images[1:50],10,5))\n",
        "plt.show()\n",
        "\n",
        "# Reconstruct and visualise the images using the vae\n",
        "print('VAE reconstruction:')\n",
        "visualise_output(images, vae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns3o755fT4tq"
      },
      "source": [
        "Interpolate in Latent Space\n",
        "----------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BQH6R1YT4tr"
      },
      "outputs": [],
      "source": [
        "vae.eval()\n",
        "\n",
        "def interpolation(lambda1, model, img1, img2):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # latent vector of first image\n",
        "        img1 = img1.to(device)\n",
        "        latent_1, _ = model.encoder(img1)\n",
        "\n",
        "        # latent vector of second image\n",
        "        img2 = img2.to(device)\n",
        "        latent_2, _ = model.encoder(img2)\n",
        "\n",
        "        # interpolation of the two latent vectors\n",
        "        inter_latent = lambda1* latent_1 + (1- lambda1) * latent_2\n",
        "\n",
        "        # reconstruct interpolated image\n",
        "        inter_image = model.decoder(inter_latent)\n",
        "        inter_image = inter_image.cpu()\n",
        "\n",
        "        return inter_image\n",
        "\n",
        "# sort part of test set by digit\n",
        "digits = [[] for _ in range(10)]\n",
        "for img_batch, label_batch in test_dataloader:\n",
        "    for i in range(img_batch.size(0)):\n",
        "        digits[label_batch[i]].append(img_batch[i:i+1])\n",
        "    if sum(len(d) for d in digits) >= 1000:\n",
        "        break;\n",
        "\n",
        "# interpolation lambdas\n",
        "lambda_range=np.linspace(0,1,10)\n",
        "\n",
        "fig, axs = plt.subplots(2,5, figsize=(15, 6))\n",
        "fig.subplots_adjust(hspace = .5, wspace=.001)\n",
        "axs = axs.ravel()\n",
        "\n",
        "for ind,l in enumerate(lambda_range):\n",
        "    inter_image=interpolation(float(l), vae, digits[7][0], digits[1][0])\n",
        "\n",
        "    inter_image = to_img(inter_image)\n",
        "\n",
        "    image = inter_image.numpy()\n",
        "\n",
        "    axs[ind].imshow(image[0,0,:,:], cmap='gray')\n",
        "    axs[ind].set_title('lambda_val='+str(round(l,1)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtqCgmhgT4ts"
      },
      "source": [
        "Sample Latent Vector from Prior (VAE as Generator)\n",
        "-------------------------------------------------\n",
        "\n",
        "A VAE can generate new digits by drawing latent vectors from the prior distribution. Although the generated digits are not perfect, they are usually better than for a non-variational Autoencoder (compare results for the 10d VAE to the results for the autoencoder).\n",
        "\n",
        "Similar to autoencoders, the manifold of latent vectors that decode to valid digits is sparser in higher-dimensional latent spaces. Increasing the weight of the KL-divergence term in the loss (increasing `variational_beta`) makes the manifold less sparse at the cost of a lower-quality reconstruction. A pre-trained model with `variational_beta = 10` is available at `./pretrained/vae_10d_beta10.pth`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9kLdAgET4ts"
      },
      "outputs": [],
      "source": [
        "vae.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # sample latent vectors from the normal distribution\n",
        "    latent = torch.randn(128, latent_dims, device=device)\n",
        "\n",
        "    # reconstruct images from the latent vectors\n",
        "    img_recon = vae.decoder(latent)\n",
        "    img_recon = img_recon.cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    show_image(torchvision.utils.make_grid(img_recon.data[:100],10,5))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTPgzJd6T4tt"
      },
      "source": [
        "Show 2D Latent Space\n",
        "---------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyFZ32PiT4tt"
      },
      "outputs": [],
      "source": [
        "# load a network that was trained with a 2d latent space\n",
        "if latent_dims != 2:\n",
        "    print('Please change the parameters to two latent dimensions.')\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # create a sample grid in 2d latent space\n",
        "    latent_x = np.linspace(-1.5,1.5,20)\n",
        "    latent_y = np.linspace(-1.5,1.5,20)\n",
        "    latents = torch.FloatTensor(len(latent_y), len(latent_x), 2)\n",
        "    for i, lx in enumerate(latent_x):\n",
        "        for j, ly in enumerate(latent_y):\n",
        "            latents[j, i, 0] = lx\n",
        "            latents[j, i, 1] = ly\n",
        "    latents = latents.view(-1, 2) # flatten grid into a batch\n",
        "\n",
        "    # reconstruct images from the latent vectors\n",
        "    latents = latents.to(device)\n",
        "    image_recon = vae.decoder(latents)\n",
        "    image_recon = image_recon.cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    show_image(torchvision.utils.make_grid(image_recon.data[:400],20,5))\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}